{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1 - Raw Data Pre-Processing</h1>\n",
    "\n",
    "This Jupyter Notebook pre-processes the raw data sampled through MPU-9250 sensors (accelerometer, gyroscope, magnetometer and temperature) and GPS, performing the following operations:\n",
    "* GPS Data Correction:\n",
    "    * Removes data from providers other than GPS\n",
    "    * Calculates the time between samples\n",
    "    * Calculates the distance between geodetic points\n",
    "    * Search for location samples with wrong time in the time sequence\n",
    "    * Renames columns to more friendly names\n",
    "    * Creates a map and with the Locations\n",
    "* MPU-9250 Settings Correction:\n",
    "    * Convert acceleration data from g to m/s^2\n",
    "    * Join settings from 0x68 and 0x69 into a file (one for right and another for left side)\n",
    "    * Renames columns to more friendly names\n",
    "* MPU-9250 Data Correction:\n",
    "    * Resamples to 100Hz\n",
    "    * Join data from 0x68 and 0x69 synchronized in the same interval into a file (one for right and another for left side)\n",
    "    * Synchronizes left and right data in the same interval\n",
    "    * Renames columns to more friendly names\n",
    "    * Convert acceleration data from g to m/s^2 \n",
    "* Synchronization of MPU and GPS data:\n",
    "    * A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Importing Packages</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import folium\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "from haversine import haversine, Unit\n",
    "from geopy.distance import geodesic\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "pd.set_option(\"float_format\", '{:0.10f}'.format)\n",
    "pd.set_option('display.max_columns', 30)\n",
    "%matplotlib inline\n",
    "%run \"1 - Raw Data Pre-Processing.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Parameters Definition</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress Bar\n",
    "load_bar = None\n",
    "\n",
    "# In seconds | 0.1s = 10Hz | 0.01s = 100Hz\n",
    "sampling_rate_interval = 0.01 \n",
    "\n",
    "# 1 = 0.1s = 10Hz | 2 = 0.01s = 100Hz\n",
    "sampling_rate_interval_decimals = len(str(sampling_rate_interval).split(\".\")[1])\n",
    "\n",
    "# Gravity value in m/s²\n",
    "gravity = 9.80665"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Utility Functions</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Date String into Timestamp Int\n",
    "def parseDateToTimestamp(value):\n",
    "    newDate = parser.parse(value)\n",
    "    return datetime.timestamp(newDate)\n",
    "\n",
    "# Transform Timestamp Int into Date String \n",
    "def parseTimestampToDate(value):\n",
    "    newDate = datetime.fromtimestamp(value)\n",
    "    return newDate.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "# Format timestamp by limiting decimal places\n",
    "def roundTimestamp(value):\n",
    "    return round(value, sampling_rate_interval_decimals)\n",
    "\n",
    "# Offset correction for incorrect times/delays between devices\n",
    "def shiftTime(data, time): # time in seconds\n",
    "    data['timestamp'] += time\n",
    "    return data\n",
    "\n",
    "# Create the folder if it doesn't exists\n",
    "def createFolder(path):\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "# Create a README.md file\n",
    "def createReadme(filePath, content):\n",
    "    \n",
    "     with open(filePath, \"w+\") as file:\n",
    "        file.write(content)\n",
    "        \n",
    "# Translate the side to English - filename\n",
    "def translateSide(side):\n",
    "    return \"right\" if side == \"direita\" else \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Importação dos Dados Brutos</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = workspace[\"settings\"]\n",
    "dataset = workspace[\"datasets\"][\"saveiro-1\"] # Dataset selection\n",
    "\n",
    "mpu_data_direita_shift_time = dataset['right_shift']\n",
    "mpu_data_esquerda_shift_time = dataset['left_shift']\n",
    "readFolder = os.path.join(settings['readFolder'], dataset['readFolder'])\n",
    "saveFolder = os.path.join(settings['saveFolder'], dataset['saveFolder'])\n",
    "createFolder(saveFolder)\n",
    "createReadme(os.path.join(saveFolder, \"README.md\"), dataset['readme'])\n",
    "\n",
    "raw_gps = pd.read_csv(os.path.join(readFolder, 'gps.csv'), float_precision=\"high\")\n",
    "\n",
    "raw_data_68_direita = pd.read_csv(os.path.join(readFolder, 'direita', 'data-set-mpu-0x68.csv'), delimiter=\";\", float_precision=\"high\")\n",
    "raw_data_69_direita = pd.read_csv(os.path.join(readFolder, 'direita', 'data-set-mpu-0x69.csv'), delimiter=\";\", float_precision=\"high\")\n",
    "settings_68_direita = pd.read_csv(os.path.join(readFolder, 'direita', 'settings-mpu-0x68.csv'), delimiter=\";\", float_precision=\"high\")\n",
    "settings_69_direita = pd.read_csv(os.path.join(readFolder, 'direita', 'settings-mpu-0x69.csv'), delimiter=\";\", float_precision=\"high\")\n",
    "\n",
    "raw_data_68_direita = shiftTime(raw_data_68_direita, mpu_data_direita_shift_time)\n",
    "raw_data_69_direita = shiftTime(raw_data_69_direita, mpu_data_direita_shift_time)\n",
    "\n",
    "raw_data_68_esquerda = pd.read_csv(os.path.join(readFolder, 'esquerda', 'data-set-mpu-0x68.csv'), delimiter=\";\", float_precision=\"high\")\n",
    "raw_data_69_esquerda = pd.read_csv(os.path.join(readFolder, 'esquerda', 'data-set-mpu-0x69.csv'), delimiter=\";\", float_precision=\"high\")\n",
    "settings_68_esquerda = pd.read_csv(os.path.join(readFolder, 'esquerda', 'settings-mpu-0x68.csv'), delimiter=\";\", float_precision=\"high\")\n",
    "settings_69_esquerda = pd.read_csv(os.path.join(readFolder, 'esquerda', 'settings-mpu-0x69.csv'), delimiter=\";\", float_precision=\"high\")\n",
    "\n",
    "raw_data_68_esquerda = shiftTime(raw_data_68_esquerda, mpu_data_esquerda_shift_time)\n",
    "raw_data_69_esquerda = shiftTime(raw_data_69_esquerda, mpu_data_esquerda_shift_time)\n",
    "\n",
    "data_gps = None\n",
    "data_direita = None\n",
    "data_esquerda = None\n",
    "\n",
    "settings_direita = None\n",
    "settings_esquerda = None\n",
    "\n",
    "iniTime = roundTimestamp(max(\n",
    "    raw_data_68_direita['timestamp'].min(), \n",
    "    raw_data_69_direita['timestamp'].min(), \n",
    "    raw_data_68_esquerda['timestamp'].min(),\n",
    "    raw_data_69_esquerda['timestamp'].min(),\n",
    "))\n",
    "\n",
    "endTime = roundTimestamp(min(\n",
    "    raw_data_68_direita['timestamp'].max(), \n",
    "    raw_data_69_direita['timestamp'].max(), \n",
    "    raw_data_68_esquerda['timestamp'].max(),\n",
    "    raw_data_69_esquerda['timestamp'].max(),\n",
    "))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>GPS Data Correction</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Data Visualization</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_gps.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_gps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_gps.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Processing</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     13,
     24,
     39,
     64,
     81
    ]
   },
   "outputs": [],
   "source": [
    "# Removes data from other providers, such as network, as they introduce outliers.\n",
    "def removeOtherProviders(data):\n",
    "    \n",
    "    iniSize = len(data)\n",
    "    data = data[data.provider == 'gps']\n",
    "    data = data.reset_index(drop=True)\n",
    "    endSize = len(data)\n",
    "    \n",
    "    print(\"Removido\", (iniSize - endSize), \"registros de outros provedores diferentes de GPS.\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Calculates the time between samples\n",
    "def calcElapsedTime(timestamps):\n",
    "    \n",
    "    elapsed = [0]\n",
    "    size = len(timestamps)\n",
    "    \n",
    "    for i in range(1, size):\n",
    "        elapsed.append(abs(timestamps[i] - timestamps[i-1]))\n",
    "    \n",
    "    return pd.Series(elapsed)\n",
    "\n",
    "# Calculates the distance between geodetic points\n",
    "def calcDistance(latitudes, longitudes):\n",
    "    \n",
    "    distance = [0]\n",
    "    size = len(latitudes)\n",
    "    \n",
    "    for i in range(1, size):\n",
    "        start = (latitudes[i-1], longitudes[i-1])\n",
    "        end = (latitudes[i], longitudes[i])\n",
    "        # dist = abs(haversine(start, end, unit='m'))\n",
    "        dist = geodesic(start, end).meters\n",
    "        distance.append(dist)\n",
    "    \n",
    "    return pd.Series(distance)\n",
    "\n",
    "# Search for location samples with wrong time in the time sequence\n",
    "def findOutliers(timestamps):\n",
    "    \n",
    "    size = len(timestamps)\n",
    "    outliers = []\n",
    "    \n",
    "    for i in range(0, size):\n",
    "        \n",
    "        if i > 0:\n",
    "            before = timestamps[i-1]\n",
    "        else:\n",
    "            before = timestamps[i]\n",
    "        \n",
    "        current = timestamps[i] \n",
    "        \n",
    "        if i < size - 1:\n",
    "            after = timestamps[i+1]\n",
    "        else:\n",
    "            after = timestamps[i]\n",
    "            \n",
    "        if before > current or current > after:\n",
    "            outliers.append(i)\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "# Main method that processes GPS data\n",
    "def processGPS(raw_gps):\n",
    "    \n",
    "    gps = removeOtherProviders(raw_gps)\n",
    "    gps.insert(0, 'timestamp', gps['time'].apply(parseDateToTimestamp), True) \n",
    "    gps = gps.drop(columns=['time'])\n",
    "    gps = gps.rename(columns={\"lat\": \"latitude\", \"lon\": \"longitude\", \"speed\": \"speed_meters_per_second\"})\n",
    "    gps.insert(len(gps.columns), 'distance_meters', calcDistance(gps['latitude'], gps['longitude']), True)\n",
    "    gps.insert(len(gps.columns), 'elapsed_time_seconds', calcElapsedTime(gps['timestamp']), True)\n",
    "    \n",
    "    outliers = findOutliers(gps['timestamp'])\n",
    "    print(\"Amount of Outliers =\", len(outliers))\n",
    "    file = os.path.join(saveFolder, \"dataset_gps.csv\")\n",
    "    gps.to_csv(file, index=False)\n",
    "    print(\"Saved in\", file)\n",
    "    return gps\n",
    "\n",
    "# Method that creates a map and shows the Locations\n",
    "def plotAndSaveMap(gps):\n",
    "    \n",
    "    locations = gps[['latitude', 'longitude']]\n",
    "    coordinates = [tuple(x) for x in locations.to_numpy()]\n",
    "\n",
    "    focolat = (gps['latitude'].min() + gps['latitude'].max()) / 2\n",
    "    focolon = (gps['longitude'].min() + gps['longitude'].max()) / 2\n",
    "    maps = folium.Map(location=[focolat, focolon], zoom_start=14)\n",
    "\n",
    "    folium.PolyLine(coordinates, color=\"blue\", weight=2.5, opacity=1).add_to(maps)\n",
    "\n",
    "    maps.save(os.path.join(saveFolder, \"map.html\"))\n",
    "    return maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gps = processGPS(raw_gps.copy())\n",
    "plotAndSaveMap(data_gps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>MPU-9250 Settings Correction</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Data Visualization</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings_68_direita.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings_69_direita.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings_68_esquerda.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings_69_esquerda.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Processing</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     10,
     12,
     39,
     66,
     93
    ]
   },
   "outputs": [],
   "source": [
    "# Convert from g to m/s^2\n",
    "def convertAccelerationSettings(data):\n",
    "    \n",
    "    data['accelerometer_bias_x'] *= gravity\n",
    "    data['accelerometer_bias_y'] *= gravity\n",
    "    data['accelerometer_bias_z'] *= gravity\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Main method that processes MPU settings\n",
    "def processSettings(data_68, data_69, side):\n",
    "    \n",
    "    data = pd.DataFrame(columns = [\n",
    "        'placement', \n",
    "        'address_mpu', \n",
    "        'address_ak',\n",
    "        'gyroscope_full_scale', \n",
    "        'accelerometer_full_scale',\n",
    "        'magnetometer_full_scale', \n",
    "        'gyroscope_resolution',\n",
    "        'accelerometer_resolution', \n",
    "        'magnetometer_resolution',\n",
    "        'gyroscope_bias_x', \n",
    "        'gyroscope_bias_y',\n",
    "        'gyroscope_bias_z',\n",
    "        'accelerometer_bias_x', \n",
    "        'accelerometer_bias_y',\n",
    "        'accelerometer_bias_z',\n",
    "        'magnetometer_factory_sensitivity_x',\n",
    "        'magnetometer_factory_sensitivity_y',\n",
    "        'magnetometer_factory_sensitivity_z',\n",
    "        'magnetometer_soft_iron_distortion_x',\n",
    "        'magnetometer_soft_iron_distortion_y',\n",
    "        'magnetometer_soft_iron_distortion_z',\n",
    "        'magnetometer_hard_iron_distortion_x',\n",
    "        'magnetometer_hard_iron_distortion_y',\n",
    "        'magnetometer_hard_iron_distortion_z'\n",
    "    ])\n",
    "    \n",
    "    data = data.append({\n",
    "        'placement': 'dashboard',\n",
    "        'address_mpu': data_69['address_mpu_master'].values[0],\n",
    "        'address_ak': data_69['address_ak'].values[0],\n",
    "        'gyroscope_full_scale': data_69['gyroscope_full_scale'].values[0],\n",
    "        'accelerometer_full_scale': data_69['accelerometer_full_scale'].values[0],\n",
    "        'magnetometer_full_scale': data_69['magnetometer_full_scale'].values[0],\n",
    "        'gyroscope_resolution': data_69['gyroscope_resolution'].values[0],\n",
    "        'accelerometer_resolution': data_69['accelerometer_resolution'].values[0],\n",
    "        'magnetometer_resolution': data_69['magnetometer_resolution'].values[0],\n",
    "        'gyroscope_bias_x': data_69['gyroscope_master_bias_x'].values[0],\n",
    "        'gyroscope_bias_y': data_69['gyroscope_master_bias_y'].values[0],\n",
    "        'gyroscope_bias_z': data_69['gyroscope_master_bias_z'].values[0],\n",
    "        'accelerometer_bias_x': data_69['accelerometer_master_bias_x'].values[0],\n",
    "        'accelerometer_bias_y': data_69['accelerometer_master_bias_y'].values[0],\n",
    "        'accelerometer_bias_z': data_69['accelerometer_master_bias_z'].values[0],\n",
    "        'magnetometer_factory_sensitivity_x': data_69['magnetometer_factory_sensitivity_x'].values[0],\n",
    "        'magnetometer_factory_sensitivity_y': data_69['magnetometer_factory_sensitivity_y'].values[0],\n",
    "        'magnetometer_factory_sensitivity_z': data_69['magnetometer_factory_sensitivity_z'].values[0],\n",
    "        'magnetometer_soft_iron_distortion_x': data_69['magnetometer_soft_iron_distortion_x'].values[0],\n",
    "        'magnetometer_soft_iron_distortion_y': data_69['magnetometer_soft_iron_distortion_y'].values[0],\n",
    "        'magnetometer_soft_iron_distortion_z': data_69['magnetometer_soft_iron_distortion_z'].values[0],\n",
    "        'magnetometer_hard_iron_distortion_x': data_69['magnetometer_hard_iron_distortion_x'].values[0],\n",
    "        'magnetometer_hard_iron_distortion_y': data_69['magnetometer_hard_iron_distortion_y'].values[0],\n",
    "        'magnetometer_hard_iron_distortion_z': data_69['magnetometer_hard_iron_distortion_z'].values[0],\n",
    "    }, ignore_index=True)\n",
    "    \n",
    "    data = data.append({\n",
    "        'placement': 'above_suspension',\n",
    "        'address_mpu': data_68['address_mpu_master'].values[0],\n",
    "        'address_ak': data_68['address_ak'].values[0],\n",
    "        'gyroscope_full_scale': data_68['gyroscope_full_scale'].values[0],\n",
    "        'accelerometer_full_scale': data_68['accelerometer_full_scale'].values[0],\n",
    "        'magnetometer_full_scale': data_68['magnetometer_full_scale'].values[0],\n",
    "        'gyroscope_resolution': data_68['gyroscope_resolution'].values[0],\n",
    "        'accelerometer_resolution': data_68['accelerometer_resolution'].values[0],\n",
    "        'magnetometer_resolution': data_68['magnetometer_resolution'].values[0],\n",
    "        'gyroscope_bias_x': data_68['gyroscope_master_bias_x'].values[0],\n",
    "        'gyroscope_bias_y': data_68['gyroscope_master_bias_y'].values[0],\n",
    "        'gyroscope_bias_z': data_68['gyroscope_master_bias_z'].values[0],\n",
    "        'accelerometer_bias_x': data_68['accelerometer_master_bias_x'].values[0],\n",
    "        'accelerometer_bias_y': data_68['accelerometer_master_bias_y'].values[0],\n",
    "        'accelerometer_bias_z': data_68['accelerometer_master_bias_z'].values[0],\n",
    "        'magnetometer_factory_sensitivity_x': data_68['magnetometer_factory_sensitivity_x'].values[0],\n",
    "        'magnetometer_factory_sensitivity_y': data_68['magnetometer_factory_sensitivity_y'].values[0],\n",
    "        'magnetometer_factory_sensitivity_z': data_68['magnetometer_factory_sensitivity_z'].values[0],\n",
    "        'magnetometer_soft_iron_distortion_x': data_68['magnetometer_soft_iron_distortion_x'].values[0],\n",
    "        'magnetometer_soft_iron_distortion_y': data_68['magnetometer_soft_iron_distortion_y'].values[0],\n",
    "        'magnetometer_soft_iron_distortion_z': data_68['magnetometer_soft_iron_distortion_z'].values[0],\n",
    "        'magnetometer_hard_iron_distortion_x': data_68['magnetometer_hard_iron_distortion_x'].values[0],\n",
    "        'magnetometer_hard_iron_distortion_y': data_68['magnetometer_hard_iron_distortion_y'].values[0],\n",
    "        'magnetometer_hard_iron_distortion_z': data_68['magnetometer_hard_iron_distortion_z'].values[0],\n",
    "    }, ignore_index=True)\n",
    "    \n",
    "    data = data.append({\n",
    "        'placement': 'below_suspension',\n",
    "        'address_mpu': data_68['address_mpu_slave'].values[0],\n",
    "        'gyroscope_full_scale': data_68['gyroscope_full_scale'].values[0],\n",
    "        'accelerometer_full_scale': data_68['accelerometer_full_scale'].values[0],\n",
    "        'magnetometer_full_scale': data_68['magnetometer_full_scale'].values[0],\n",
    "        'gyroscope_resolution': data_68['gyroscope_resolution'].values[0],\n",
    "        'accelerometer_resolution': data_68['accelerometer_resolution'].values[0],\n",
    "        'magnetometer_resolution': data_68['magnetometer_resolution'].values[0],\n",
    "        'gyroscope_bias_x': data_68['gyroscope_slave_bias_x'].values[0],\n",
    "        'gyroscope_bias_y': data_68['gyroscope_slave_bias_y'].values[0],\n",
    "        'gyroscope_bias_z': data_68['gyroscope_slave_bias_z'].values[0],\n",
    "        'accelerometer_bias_x': data_68['accelerometer_slave_bias_x'].values[0],\n",
    "        'accelerometer_bias_y': data_68['accelerometer_slave_bias_y'].values[0],\n",
    "        'accelerometer_bias_z': data_68['accelerometer_slave_bias_z'].values[0]\n",
    "    }, ignore_index=True)\n",
    "    \n",
    "    # Acceleration parse\n",
    "    data = convertAccelerationSettings(data)\n",
    "    \n",
    "    data = data.reset_index(drop=True)\n",
    "    \n",
    "    file = os.path.join(saveFolder, \"dataset_settings_\" + translateSide(side) + \".csv\")\n",
    "    data.to_csv(file, index=False)\n",
    "    print(\"Saved in\", file)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings_direita = processSettings(settings_68_direita.copy(), settings_69_direita.copy(), 'direita')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings_esquerda = processSettings(settings_68_esquerda.copy(), settings_69_esquerda.copy(), 'esquerda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>MPU-9250 Data Correction</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Data Visualization</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_68_direita.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_69_direita.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_68_esquerda.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_69_esquerda.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_68_direita.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_69_direita.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_68_esquerda.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_69_esquerda.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Processing</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     14,
     52,
     59,
     79,
     94,
     158,
     175
    ]
   },
   "outputs": [],
   "source": [
    "# Corrects rounding and duplication of timestamps\n",
    "def fixTimestamps(data):\n",
    "    \n",
    "    # Arredonda Timestamps\n",
    "    data['timestamp'] = data['timestamp'].apply(roundTimestamp)\n",
    "    \n",
    "    # Agrupa Timestamps - Média\n",
    "    data = data.groupby([\"timestamp\"], as_index=False).mean()\n",
    "    \n",
    "    data = data.reset_index(drop=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Sequences samples from start to finish, at a fixed step interval\n",
    "def sequencing(data):\n",
    "\n",
    "    index = -1\n",
    "    series = None\n",
    "    timestamp = 0\n",
    "    ini = iniTime\n",
    "    end = endTime\n",
    "    new_points = []\n",
    "    result = []\n",
    "    \n",
    "    while(timestamp < ini):\n",
    "        index += 1\n",
    "        series = data.iloc[index].copy()\n",
    "        timestamp = roundTimestamp(series['timestamp'])\n",
    "\n",
    "    while ini <= end and index < len(data):\n",
    "        \n",
    "        series = data.iloc[index].copy()\n",
    "        timestamp = roundTimestamp(series['timestamp'])\n",
    "            \n",
    "        if ini == timestamp:\n",
    "            index += 1 \n",
    "\n",
    "        else:\n",
    "            series = (result[-1].copy() + series) / 2.0\n",
    "            new_points.append(len(result))\n",
    "        \n",
    "        series['timestamp'] = ini\n",
    "        result.append(series)\n",
    "        ini = roundTimestamp(ini + sampling_rate_interval)\n",
    "        \n",
    "        load_bar.update(1)\n",
    "\n",
    "    dataFrame = pd.DataFrame(result)\n",
    "    dataFrame = dataFrame.reset_index(drop=True)\n",
    "    return new_points, dataFrame\n",
    "\n",
    "# Joins the 0x68 and 0x69 data into a file with new column labels\n",
    "def mergeData(data_68, data_69):\n",
    "    \n",
    "    # Changing column names\n",
    "\n",
    "    data_68 = data_68.rename(columns={\n",
    "        'master_acc_x': 'acc_x_above_suspension',\n",
    "        'master_acc_y': 'acc_y_above_suspension',\n",
    "        'master_acc_z': 'acc_z_above_suspension',\n",
    "        'master_gyro_x': 'gyro_x_above_suspension',\n",
    "        'master_gyro_y': 'gyro_y_above_suspension',\n",
    "        'master_gyro_z': 'gyro_z_above_suspension',\n",
    "        'slave_acc_x': 'acc_x_below_suspension',\n",
    "        'slave_acc_y': 'acc_y_below_suspension',\n",
    "        'slave_acc_z': 'acc_z_below_suspension',\n",
    "        'slave_gyro_x': 'gyro_x_below_suspension',\n",
    "        'slave_gyro_y': 'gyro_y_below_suspension',\n",
    "        'slave_gyro_z': 'gyro_z_below_suspension',\n",
    "        'mag_x': 'mag_x_above_suspension',\n",
    "        'mag_y': 'mag_y_above_suspension',\n",
    "        'mag_z': 'mag_z_above_suspension',\n",
    "        'master_temp': 'temp_above_suspension',\n",
    "        'slave_temp': 'temp_below_suspension'\n",
    "    })\n",
    "\n",
    "    data_69 = data_69.rename(columns={\n",
    "        'master_acc_x': 'acc_x_dashboard',\n",
    "        'master_acc_y': 'acc_y_dashboard',\n",
    "        'master_acc_z': 'acc_z_dashboard',\n",
    "        'master_gyro_x': 'gyro_x_dashboard',\n",
    "        'master_gyro_y': 'gyro_y_dashboard',\n",
    "        'master_gyro_z': 'gyro_z_dashboard',\n",
    "        'mag_x': 'mag_x_dashboard',\n",
    "        'mag_y': 'mag_y_dashboard',\n",
    "        'mag_z': 'mag_z_dashboard',\n",
    "        'master_temp': 'temp_dashboard'\n",
    "    })\n",
    "    \n",
    "    # Deleting unused columns\n",
    "    \n",
    "    data_69 = data_69.drop(columns=[\n",
    "        'slave_acc_x', \n",
    "        'slave_acc_y', \n",
    "        'slave_acc_z', \n",
    "        'slave_gyro_x', \n",
    "        'slave_gyro_y', \n",
    "        'slave_gyro_z',\n",
    "        'slave_temp'\n",
    "    ])\n",
    "    \n",
    "    # Merge 0x68 and 0x69 for new dataframe\n",
    "\n",
    "    fieldsSource = {\n",
    "        'timestamp': data_68,\n",
    "\n",
    "        'acc_x_dashboard': data_69,\n",
    "        'acc_y_dashboard': data_69,\n",
    "        'acc_z_dashboard': data_69,\n",
    "        \n",
    "        'acc_x_above_suspension': data_68,\n",
    "        'acc_y_above_suspension': data_68,\n",
    "        'acc_z_above_suspension': data_68,\n",
    "        \n",
    "        'acc_x_below_suspension': data_68,\n",
    "        'acc_y_below_suspension': data_68,\n",
    "        'acc_z_below_suspension': data_68,\n",
    "\n",
    "        'gyro_x_dashboard': data_69,\n",
    "        'gyro_y_dashboard': data_69,\n",
    "        'gyro_z_dashboard': data_69,\n",
    "        \n",
    "        'gyro_x_above_suspension': data_68,\n",
    "        'gyro_y_above_suspension': data_68,\n",
    "        'gyro_z_above_suspension': data_68,\n",
    "        \n",
    "        'gyro_x_below_suspension': data_68,\n",
    "        'gyro_y_below_suspension': data_68,\n",
    "        'gyro_z_below_suspension': data_68,\n",
    "\n",
    "        'mag_x_dashboard': data_69,\n",
    "        'mag_y_dashboard': data_69,\n",
    "        'mag_z_dashboard': data_69,\n",
    "        \n",
    "        'mag_x_above_suspension': data_68,\n",
    "        'mag_y_above_suspension': data_68,\n",
    "        'mag_z_above_suspension': data_68,\n",
    "        \n",
    "        'temp_dashboard': data_69,\n",
    "        'temp_above_suspension': data_68,\n",
    "        'temp_below_suspension': data_68\n",
    "    }\n",
    "\n",
    "    index = 0\n",
    "    data = pd.DataFrame()\n",
    "    \n",
    "    for key in fieldsSource:\n",
    "        dataSource = fieldsSource[key]\n",
    "        column = dataSource[key]        \n",
    "        data.insert(index, column.name, column, True)\n",
    "        index += 1\n",
    "\n",
    "    return data\n",
    "\n",
    "# Convert from g to m/s^2\n",
    "def convertAcceleration(data):\n",
    "        \n",
    "    data['acc_x_dashboard'] *= gravity\n",
    "    data['acc_y_dashboard'] *= gravity\n",
    "    data['acc_z_dashboard'] *= gravity\n",
    "    \n",
    "    data['acc_x_above_suspension'] *= gravity\n",
    "    data['acc_y_above_suspension'] *= gravity\n",
    "    data['acc_z_above_suspension'] *= gravity\n",
    "    \n",
    "    data['acc_x_below_suspension'] *= gravity\n",
    "    data['acc_y_below_suspension'] *= gravity\n",
    "    data['acc_z_below_suspension'] *= gravity\n",
    "    \n",
    "    return data\n",
    "    \n",
    "# Main method that processes the MPU data\n",
    "def processMPU(data_68, data_69, side):\n",
    "    \n",
    "    print(\"Processing data from MPU on the\", side, \"side\")\n",
    "    print(\"Ini\", iniTime)\n",
    "    print(\"End\", endTime)\n",
    "    print(\"Step\", sampling_rate_interval)\n",
    "    print(\"Samples\", round((endTime - iniTime) / sampling_rate_interval) + 1)\n",
    "    \n",
    "    # Correcting Timestamps\n",
    "    data_68 = fixTimestamps(data_68)\n",
    "    data_69 = fixTimestamps(data_69)\n",
    "    \n",
    "    # Pairing of data 0x68 and 0x69\n",
    "    points_68, data_68 = sequencing(data_68)\n",
    "    points_69, data_69 = sequencing(data_69)\n",
    "    \n",
    "    # Joining the data 0x68 and 0x69\n",
    "    data = mergeData(data_68, data_69)\n",
    "    \n",
    "    # Converts acceleration from g to m/s^2\n",
    "    data = convertAcceleration(data)\n",
    "\n",
    "    # Saving\n",
    "    file = os.path.join(saveFolder, 'dataset_mpu_' + translateSide(side) + '.csv')\n",
    "    data.to_csv(file, index=False)\n",
    "    print(\"Saved in \", file)\n",
    "    \n",
    "    print(\"Length Data 0x68 =\", len(data_68))\n",
    "    print(\"Length Data 0x69 =\", len(data_69))\n",
    "    print(\"\\n\", \"Length New Points 0x68 =\", len(points_68))\n",
    "    print(points_68)\n",
    "    print(\"\\n\",\"Length New Points 0x69 =\", len(points_69))\n",
    "    print(points_69)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_progress = (round((endTime - iniTime) / sampling_rate_interval) * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "load_bar = tqdm_notebook(total=count_progress)\n",
    "data_direita = processMPU(raw_data_68_direita.copy(), raw_data_69_direita.copy(), 'direita')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "load_bar = tqdm_notebook(total=count_progress)\n",
    "data_esquerda = processMPU(raw_data_68_esquerda.copy(), raw_data_69_esquerda.copy(), 'esquerda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Synchronization of MPU and GPS data</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Data Visualization</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_direita.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_esquerda.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gps.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Processing</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     12
    ]
   },
   "outputs": [],
   "source": [
    "# Maps GPS data based on the MPU sample timestamp\n",
    "def mergeMPUGPS(data_mpu, data_gps, side):\n",
    "    \n",
    "    columns = ['timestamp', 'latitude', 'longitude', 'speed_meters_per_second']\n",
    "    result = []\n",
    "    \n",
    "    indexMPU = 0\n",
    "    indexGPS = 0\n",
    "    minusIndex = 2\n",
    "    timestampMPU = 0\n",
    "    timestampGPS = 0\n",
    "    \n",
    "    while indexMPU < len(data_mpu):\n",
    "        \n",
    "        timestampMPU = data_mpu.loc[indexMPU, 'timestamp']\n",
    "        \n",
    "        while timestampGPS <= timestampMPU and indexGPS < len(data_gps):\n",
    "            timestampGPS = data_gps.loc[indexGPS, 'timestamp']\n",
    "            indexGPS += 1\n",
    "            \n",
    "        if indexGPS == len(data_gps) and timestampMPU >= timestampGPS:\n",
    "            minusIndex = 1\n",
    "            \n",
    "        series = data_gps.loc[indexGPS - minusIndex, columns] \n",
    "        result.append(series)\n",
    "        indexMPU += 1\n",
    "        \n",
    "        load_bar.update(1)\n",
    "        \n",
    "    data = pd.DataFrame(result)\n",
    "    data = data.reset_index(drop=True)\n",
    "    data = data.rename(columns={ 'timestamp': 'timestamp_gps'})\n",
    "    data = data.rename(columns={ 'speed_meters_per_second': 'speed'})\n",
    "    \n",
    "    series = data['timestamp_gps']\n",
    "    data_mpu.insert(len(data_mpu.columns), series.name, series, True)\n",
    "    series = data['latitude']\n",
    "    data_mpu.insert(len(data_mpu.columns), series.name, series, True)\n",
    "    series = data['longitude']\n",
    "    data_mpu.insert(len(data_mpu.columns), series.name, series, True)\n",
    "    series = data['speed']\n",
    "    data_mpu.insert(len(data_mpu.columns), series.name, series, True)\n",
    "    \n",
    "    file = os.path.join(saveFolder, \"dataset_gps_mpu_\" + translateSide(side) + \".csv\")\n",
    "    data_mpu.to_csv(file, index=False)\n",
    "    print(\"Saved in \", file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_bar = tqdm_notebook(total=len(data_direita))\n",
    "mergeMPUGPS(data_direita.copy(), data_gps.copy(), \"direita\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_bar = tqdm_notebook(total=len(data_esquerda))\n",
    "mergeMPUGPS(data_esquerda.copy(), data_gps.copy(), \"esquerda\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
